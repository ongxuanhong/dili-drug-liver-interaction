{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from joblib import dump\n",
    "from itertools import combinations\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv(\"data/training_class.CSV\")\n",
    "pd_train[\"label\"] = pd_train[\"Class\"].apply(lambda x: 1 if x == \"Hepatotoxicity\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = pd.read_csv(\"data/testing_class.CSV\")\n",
    "pd_test[\"label\"] = pd_test[\"Class\"].apply(lambda x: 1 if x == \"Hepatotoxicity\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  992\n",
      "Val size:  249\n",
      "Test size:  286\n"
     ]
    }
   ],
   "source": [
    "# train val split with sklearn\n",
    "features = pd_train.columns[1:-1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    pd_train[features], pd_train[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "X_test = pd_test[features]\n",
    "y_test = pd_test[\"label\"]\n",
    "\n",
    "print(\"Train size: \", X_train.shape[0])\n",
    "print(\"Val size: \", X_val.shape[0])\n",
    "print(\"Test size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the actual column names in the dataset\n",
    "actual_columns = X_train.columns\n",
    "\n",
    "# Update the fingerprints dictionary with existing column names\n",
    "feature_groups = {\n",
    "    \"FP\": [col for col in actual_columns if col.startswith(\"FP\")],\n",
    "    \"ExtFP\": [col for col in actual_columns if col.startswith(\"ExtFP\")],\n",
    "    \"EstateFP\": [col for col in actual_columns if col.startswith(\"EstateFP\")],\n",
    "    \"GraphFP\": [col for col in actual_columns if col.startswith(\"GraphFP\")],\n",
    "    \"MACCSFP\": [col for col in actual_columns if col.startswith(\"MACCSFP\")],\n",
    "    \"PubchemFP\": [col for col in actual_columns if col.startswith(\"PubchemFP\")],\n",
    "    \"SubFP\": [col for col in actual_columns if col.startswith(\"SubFP\")],\n",
    "    \"SubFPC\": [col for col in actual_columns if col.startswith(\"SubFPC\")],\n",
    "    \"KRFP\": [col for col in actual_columns if col.startswith(\"KRFP\")],\n",
    "    \"KRFPC\": [col for col in actual_columns if col.startswith(\"KRFPC\")],\n",
    "    \"AD2D\": [col for col in actual_columns if col.startswith(\"AD2D\")],\n",
    "    \"APC2D\": [col for col in actual_columns if col.startswith(\"APC2D\")],\n",
    "}\n",
    "len(feature_groups[\"FP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "def create_directories(iteration):\n",
    "    model_dir = f\"models/iteration_{iteration}\"\n",
    "    log_dir = f\"logs/iteration_{iteration}\"\n",
    "    feat_imp_dir = f\"feat_imp/iteration_{iteration}\"\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(feat_imp_dir, exist_ok=True)\n",
    "\n",
    "    return model_dir, log_dir, feat_imp_dir\n",
    "\n",
    "\n",
    "# Prepare the dataset by dropping feature groups\n",
    "def drop_feature_groups(X, feature_groups, drop_list):\n",
    "    features_to_drop = [item for i in drop_list for item in feature_groups[i]]\n",
    "    return X.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model and save the logs, model, and feature importance\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, drop_list_str):\n",
    "    param_init = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"max_depth\": 4,\n",
    "        \"n_estimators\": 300,\n",
    "        \"learning_rate\": 0.025,\n",
    "        \"subsample\": 0.7,\n",
    "        \"colsample_bytree\": 0.3,\n",
    "        \"colsample_bylevel\": 0.5,\n",
    "        # \"silent\": True,\n",
    "        \"n_jobs\": 14,\n",
    "        #\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"grow_policy\": \"lossguide\",\n",
    "        #\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"early_stopping_rounds\": 100,\n",
    "    }\n",
    "\n",
    "    param_fit = {\n",
    "        \"verbose\": 100,\n",
    "        \"eval_set\": [(X_train, y_train), (X_val, y_val), (X_test, y_test)],\n",
    "    }\n",
    "\n",
    "    # Train 5 times for each drop set\n",
    "    for run in range(5):\n",
    "        print(f\"Iteration {run}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        model_dir, log_dir, feat_imp_dir = create_directories(run)\n",
    "\n",
    "        # Define the XGBoost model\n",
    "        param_init[\"random_state\"] = run + 42\n",
    "        model = xgb.XGBClassifier(**param_init)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, **param_fit)\n",
    "\n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "        y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate log loss and AUC\n",
    "        train_logloss = log_loss(y_train, y_train_pred)\n",
    "        val_logloss = log_loss(y_val, y_val_pred)\n",
    "        test_logloss = log_loss(y_test, y_test_pred)\n",
    "        train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f\"{model_dir}/xgboost_drop_{drop_list_str}.joblib\"\n",
    "        dump(model, model_path)\n",
    "\n",
    "        # Save evaluation metrics\n",
    "        log_path = f\"{log_dir}/xgboost_drop_{drop_list_str}.txt\"\n",
    "        with open(log_path, \"w\") as f:\n",
    "            f.write(f\"Train Logloss: {train_logloss}\\n\")\n",
    "            f.write(f\"Validation Logloss: {val_logloss}\\n\")\n",
    "            f.write(f\"Test Logloss: {test_logloss}\\n\")\n",
    "            f.write(f\"Train AUC: {train_auc}\\n\")\n",
    "            f.write(f\"Validation AUC: {val_auc}\\n\")\n",
    "            f.write(f\"Test AUC: {test_auc}\\n\")\n",
    "\n",
    "        # Save feature importance\n",
    "        feat_imp_path = f\"{feat_imp_dir}/xgboost_drop_{drop_list_str}.csv\"\n",
    "        feature_importances = pd.DataFrame(\n",
    "            {\"Feature\": X_train.columns, \"Importance\": model.feature_importances_}\n",
    "        )\n",
    "        feature_importances.to_csv(feat_imp_path, index=False)\n",
    "\n",
    "        # save time\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        log_path = f\"{log_dir}/running_time_{drop_list_str}.txt\"\n",
    "        with open(log_path, \"a\") as f:\n",
    "            f.write(f\"Time taken: {end_time - start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299]\tvalidation_0-auc:0.98314\tvalidation_1-auc:0.77352\tvalidation_2-auc:0.90115\n",
      "Time taken: 114.17 seconds\n",
      "Iteration 4\n",
      "[0]\tvalidation_0-auc:0.71988\tvalidation_1-auc:0.66054\tvalidation_2-auc:0.57699\n",
      "[100]\tvalidation_0-auc:0.94215\tvalidation_1-auc:0.77260\tvalidation_2-auc:0.88395\n",
      "[200]\tvalidation_0-auc:0.97019\tvalidation_1-auc:0.77787\tvalidation_2-auc:0.89245\n",
      "[299]\tvalidation_0-auc:0.98249\tvalidation_1-auc:0.77827\tvalidation_2-auc:0.90017\n",
      "Time taken: 121.43 seconds\n",
      "Dropping: MACCSFP_KRFP\n",
      "Iteration 0\n",
      "[0]\tvalidation_0-auc:0.70641\tvalidation_1-auc:0.68578\tvalidation_2-auc:0.64427\n",
      "[100]\tvalidation_0-auc:0.94160\tvalidation_1-auc:0.78163\tvalidation_2-auc:0.87929\n"
     ]
    }
   ],
   "source": [
    "# Run backward selection with XGBoost\n",
    "# Feature group names and sizes\n",
    "feature_group_names = list(feature_groups.keys())\n",
    "\n",
    "max_num_groups_to_drop = int(len(feature_group_names) / 2)\n",
    "for num_groups_to_drop in range(1, max_num_groups_to_drop):\n",
    "    print(f\"Number of groups to drop: {num_groups_to_drop}\")\n",
    "\n",
    "    # Get all combinations of feature groups to drop\n",
    "    for drop_indices in combinations(\n",
    "        range(len(feature_group_names)), num_groups_to_drop\n",
    "    ):\n",
    "        drop_list = [feature_group_names[i] for i in drop_indices]\n",
    "        drop_list_str = \"_\".join(drop_list)\n",
    "        print(f\"Dropping: {drop_list_str}\")\n",
    "\n",
    "        # Drop the selected feature groups\n",
    "        X_train_dropped = drop_feature_groups(X_train, feature_groups, drop_list)\n",
    "        X_val_dropped = drop_feature_groups(X_val, feature_groups, drop_list)\n",
    "        X_test_dropped = drop_feature_groups(X_test, feature_groups, drop_list)\n",
    "\n",
    "        # Train and evaluate the model with the dropped feature groups\n",
    "        train_xgboost(\n",
    "            X_train_dropped,\n",
    "            y_train,\n",
    "            X_val_dropped,\n",
    "            y_val,\n",
    "            X_test_dropped,\n",
    "            y_test,\n",
    "            drop_list_str,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dili",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
