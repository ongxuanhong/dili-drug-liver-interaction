{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModernBERT example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pd_train\n",
    "pd_train = pd.read_csv(\"data_smiles/Training_Group.csv\")\n",
    "pd_train[\"label\"] = pd_train[\"Liver\"].apply(lambda x: 1 if x == \"Hepatotoxicity\" else 0)\n",
    "print(pd_train.shape)\n",
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1241.000000\n",
       "mean       63.667204\n",
       "std        61.782302\n",
       "min         8.000000\n",
       "25%        36.000000\n",
       "50%        48.000000\n",
       "75%        68.000000\n",
       "max       748.000000\n",
       "Name: smiles_length, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMILES length column\n",
    "pd_train[\"smiles_length\"] = pd_train[\"Smiles\"].apply(lambda x: len(x))\n",
    "pd_train[\"smiles_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pd_test\n",
    "pd_test = pd.read_csv(\"data_smiles/Testing_Group.csv\")\n",
    "pd_test[\"label\"] = pd_test[\"Liver\"].apply(lambda x: 1 if x == \"Hepatotoxicity\" else 0)\n",
    "print(pd_test.shape)\n",
    "pd_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    286.000000\n",
       "mean      54.370629\n",
       "std       39.480251\n",
       "min       10.000000\n",
       "25%       32.000000\n",
       "50%       44.000000\n",
       "75%       60.000000\n",
       "max      284.000000\n",
       "Name: smiles_length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMILES length column\n",
    "pd_test[\"smiles_length\"] = pd_test[\"Smiles\"].apply(lambda x: len(x))\n",
    "pd_test[\"smiles_length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to .npy files\n",
    "np.save(\"modern_bert_data/X_train.npy\", X_train)\n",
    "np.save(\"modern_bert_data/X_test.npy\", X_test)\n",
    "np.save(\"modern_bert_data/y_train.npy\", y_train)\n",
    "np.save(\"modern_bert_data/y_test.npy\", y_test)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1241, 50368)\n",
      "X_test shape: (286, 50368)\n",
      "y_train shape: (1241,)\n",
      "y_test shape: (286,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train = np.load(\"modern_bert_data/X_train.npy\")\n",
    "X_test = np.load(\"modern_bert_data/X_test.npy\")\n",
    "y_train = np.load(\"modern_bert_data/y_train.npy\")\n",
    "y_test = np.load(\"modern_bert_data/y_test.npy\")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Find optimal threshold based on sensitivity >= 0.7 or best F1 score.\n",
    "    \"\"\"\n",
    "    best_threshold = 0.5\n",
    "    best_metrics = {\n",
    "        \"accuracy\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"sensitivity\": 0,\n",
    "        \"specificity\": 0,\n",
    "        \"f1\": 0,\n",
    "    }\n",
    "\n",
    "    for threshold in np.arange(0.0, 1.0, 0.01):\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        if sensitivity >= 0.7 or f1 > best_metrics[\"f1\"]:\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                \"auc\": auc,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"sensitivity\": sensitivity,\n",
    "                \"specificity\": specificity,\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "\n",
    "    return best_threshold, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# max=748, min=8, avg=54/63\n",
    "max_seq_length = 748\n",
    "max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n",
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 1241/1241 [00:00<00:00, 11605.93 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 286/286 [00:00<00:00, 11927.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "  # Tokenize the texts\n",
    "  result = tokenizer(\n",
    "      examples[\"Smiles\"],\n",
    "      padding=\"max_length\",\n",
    "      max_length=max_seq_length,\n",
    "      truncation=True\n",
    "  )\n",
    "  result[\"label\"] = examples[\"label\"]\n",
    "  return result\n",
    "\n",
    "# \n",
    "pd_train_hf = Dataset.from_pandas(pd_train)\n",
    "pd_test_hf = Dataset.from_pandas(pd_test)\n",
    "\n",
    "pd_train_hf_processed = pd_train_hf.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "pd_test_hf_processed = pd_test_hf.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"finetuning_task\": \"text-classification\",\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.47.1\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 2\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=\"text-classification\"\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions , axis=1)\n",
    "  result = metric.compute(predictions=predictions, references=labels)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56107/244675088.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"save_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=20,\n",
    "    per_device_eval_batch_size=20,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],  # Only use tensorboard\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=pd_train_hf_processed,\n",
    "    eval_dataset=pd_test_hf_processed,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 09:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585658</td>\n",
       "      <td>0.772727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>0.741259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.568090</td>\n",
       "      <td>0.741259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.536167</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587361</td>\n",
       "      <td>0.702797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614004</td>\n",
       "      <td>0.695804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744998</td>\n",
       "      <td>0.604895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.768538</td>\n",
       "      <td>0.608392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.681831</td>\n",
       "      <td>0.688811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.703761</td>\n",
       "      <td>0.678322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 15s, sys: 5.09 s, total: 9min 20s\n",
      "Wall time: 9min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.5322345612541077, metrics={'train_runtime': 560.5552, 'train_samples_per_second': 22.139, 'train_steps_per_second': 1.124, 'total_flos': 1643920417320960.0, 'train_loss': 0.5322345612541077, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.5361668467521667,\n",
       " 'test_accuracy': 0.7622377622377622,\n",
       " 'test_runtime': 4.2726,\n",
       " 'test_samples_per_second': 66.938,\n",
       " 'test_steps_per_second': 3.511}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = trainer.predict(pd_test_hf_processed) \n",
    "outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main Metrics:\n",
      "auc: 0.6546\n",
      "precision: 0.8283\n",
      "recall: 0.8733\n",
      "f1: 0.8502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_detailed_metrics(outputs, labels):\n",
    "    # Get predictions and probabilities\n",
    "    logits = outputs.predictions\n",
    "    # convert logits to tensor\n",
    "    logits = torch.tensor(logits)\n",
    "\n",
    "    probs = logits.softmax(axis=1)\n",
    "    pred_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "    # labels, pred_labels to numpy\n",
    "    labels = np.array(labels)\n",
    "    pred_labels = pred_labels.numpy()\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Binary classification case\n",
    "    if logits.shape[1] == 2:\n",
    "        metrics['auc'] = roc_auc_score(labels, probs[:, 1])\n",
    "        metrics['precision'] = precision_score(labels, pred_labels)\n",
    "        metrics['recall'] = recall_score(labels, pred_labels)\n",
    "        metrics['f1'] = f1_score(labels, pred_labels)\n",
    "    \n",
    "    # Multi-class case\n",
    "    else:\n",
    "        metrics['auc'] = roc_auc_score(labels, probs, multi_class='ovr')\n",
    "        metrics['precision'] = precision_score(labels, pred_labels, average='macro')\n",
    "        metrics['recall'] = recall_score(labels, pred_labels, average='macro')\n",
    "        metrics['f1'] = f1_score(labels, pred_labels, average='macro')\n",
    "    \n",
    "    # Get detailed classification report\n",
    "    class_report = classification_report(labels, pred_labels, output_dict=True)\n",
    "    \n",
    "    return metrics, class_report\n",
    "\n",
    "# Use with trainer outputs\n",
    "outputs = trainer.predict(pd_test_hf_processed)\n",
    "metrics, detailed_report = evaluate_detailed_metrics(outputs, pd_test_hf_processed['label'])\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMain Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# print(\"\\nDetailed Classification Report:\")\n",
    "# for class_name, values in detailed_report.items():\n",
    "#     if isinstance(values, dict):\n",
    "#         print(f\"\\nClass {class_name}:\")\n",
    "#         for metric, value in values.items():\n",
    "#             if isinstance(value, float):\n",
    "#                 print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Find optimal threshold based on sensitivity >= 0.7 or best F1 score.\n",
    "    \"\"\"\n",
    "    best_threshold = 0.5\n",
    "    best_metrics = {\n",
    "        \"accuracy\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"sensitivity\": 0,\n",
    "        \"specificity\": 0,\n",
    "        \"f1\": 0,\n",
    "    }\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    for threshold in np.arange(0.0, 1.0, 0.01):\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        if sensitivity >= 0.7 or f1 > best_metrics[\"f1\"]:\n",
    "            best_threshold = threshold\n",
    "            best_metrics = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"sensitivity\": sensitivity,\n",
    "                \"specificity\": specificity,\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "\n",
    "    return best_threshold, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56107/4058325157.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y_pred_proba = np.array(y_pred_proba)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy        : 0.6608\n",
      "Auc             : 0.6546\n",
      "Precision       : 0.8298\n",
      "Recall          : 0.7059\n",
      "F1              : 0.7628\n",
      "Sensitivity     : 0.7059\n",
      "Specificity     : 0.5077\n",
      "\n",
      "Confusion Matrix:\n",
      "--------------------------------------------------\n",
      "true_negatives  : 33\n",
      "false_positives : 32\n",
      "false_negatives : 65\n",
      "true_positives  : 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/hongong/anaconda3/envs/dili-drug-liver-interaction/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/tmp/ipykernel_56107/2109038839.py:54: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  pred_probs = np.array(pred_probs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "def compute_binary_metrics(pred_probs, pred_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Compute comprehensive binary classification metrics\n",
    "    \"\"\"\n",
    "    # labels, pred_labels to numpy\n",
    "    true_labels = np.array(true_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "        \"auc\": roc_auc_score(true_labels, pred_probs),\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "        \"sensitivity\": tp / (tp + fn),  # Same as recall\n",
    "        \"specificity\": tn / (tn + fp),\n",
    "        \"confusion_matrix\": {\n",
    "            \"true_negatives\": tn,\n",
    "            \"false_positives\": fp,\n",
    "            \"false_negatives\": fn,\n",
    "            \"true_positives\": tp\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Process trainer.predict outputs\n",
    "outputs = trainer.predict(pd_test_hf_processed)\n",
    "logits = outputs.predictions\n",
    "true_labels = outputs.label_ids\n",
    "\n",
    "# Get probabilities and predicted labels\n",
    "logits = torch.tensor(logits)\n",
    "probs = logits.softmax(axis=1)\n",
    "pred_probs = probs[:, 1]  # For binary classification, take probability of positive class\n",
    "# pred_labels = np.argmax(logits, axis=1)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_threshold, best_metrics = find_optimal_threshold(true_labels, pred_probs)\n",
    "pred_probs = np.array(pred_probs)\n",
    "pred_labels = (pred_probs >= best_threshold).astype(int)\n",
    "\n",
    "# Compute all metrics\n",
    "all_metrics = compute_binary_metrics(pred_probs, pred_labels, true_labels)\n",
    "\n",
    "# Print results in a formatted way\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "for metric, value in all_metrics.items():\n",
    "    if metric != \"confusion_matrix\":\n",
    "        print(f\"{metric.capitalize():15} : {value:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in all_metrics[\"confusion_matrix\"].items():\n",
    "    print(f\"{key:15} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dili-drug-liver-interaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
